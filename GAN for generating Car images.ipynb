{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing The Right Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tt\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "%matplotlib inline\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting our Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../input/stanford-cars-dataset/cars_train\"\n",
    "image_size = 256\n",
    "batch_size = 64\n",
    "normstats = (0.,0.,0.),(1.,1.,1.)\n",
    "transforms = tt.Compose([tt.Resize(image_size),#resize to make things uniform\n",
    "                        tt.CenterCrop(image_size),#cropping to the center to avoid distortion\n",
    "                        tt.ToTensor(),#to a tensor\n",
    "                        tt.Normalize(*normstats)#normalizing in order to increase effectiveness of our GAN\n",
    "                        ])\n",
    "dataset = ImageFolder(data_path, transform = transforms)\n",
    "img, _ = dataset[0]\n",
    "plt.imshow(img.permute((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img_tensors):\n",
    "    return img_tensors * normstats[1][0] + normstats[0][0]\n",
    "\n",
    "def show_batch(dl):#just to show one batch of our data\n",
    "    for img, _ in dl:\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        ax.set_xticks([]);ax.set_yticks([])\n",
    "        ax.imshow(torchvision.utils.make_grid(img[:64],nrow = 8).permute(1,2,0))\n",
    "        break\n",
    "\n",
    "def show_images(images):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.set_xticks([]),ax.set_yticks([])\n",
    "    ax.imshow(torchvision.utils.make_grid(denorm(images.detach()[:64]),nrow = 8).permute(1,2,0))\n",
    "\n",
    "dataload = DataLoader(dataset,batch_size,num_workers = 4,shuffle = True, pin_memory=True)#makes our data into batches\n",
    "show_batch(dataload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = find_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data,device):\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    else:\n",
    "        return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderDeviced():\n",
    "    def __init__(self,data,device):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.data:\n",
    "            yield to_device(b,self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataload = DataloaderDeviced(dataload,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our Discirminator\n",
    "we have input images of size 3x256x256\n",
    "we are now trying to define a descriminator network that would differentiate between fake and real images acting as the loss function to our generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriminator = nn.Sequential(\n",
    "    #input size being of 3 channels, 256x256\n",
    "    nn.Conv2d(3, 32 ,kernel_size = 3, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #output size being of 32 channels, 128x128\n",
    "    \n",
    "    nn.Conv2d(32,64,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #out 64x64x64\n",
    "    \n",
    "    nn.Conv2d(64,128,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.1, inplace = True),\n",
    "    #out 128x32x32\n",
    "    \n",
    "    nn.Conv2d(128,256,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.1,inplace = True),\n",
    "    #out 256x16x16\n",
    "    \n",
    "    nn.Conv2d(256,512, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.1, inplace = True),\n",
    "    #out 512x8x8\n",
    "    \n",
    "    nn.Conv2d(512,1024, kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.LeakyReLU(0.1,inplace = True),\n",
    "    #out 1024x4x4\n",
    "    \n",
    "    nn.Conv2d(1024,1,kernel_size = 4,stride = 1, padding = 0, bias = False),\n",
    "    #out 1x1x1\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    nn.Sigmoid(),\n",
    "    #final activation for T/F\n",
    ")\n",
    "\n",
    "descriminator=to_device(descriminator,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our Generator\n",
    "taking in a latent vector of size 128, we're going to generate an image of size 256x256.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sz = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    #latent in 128x1x1\n",
    "    nn.ConvTranspose2d(128,1024,kernel_size = 4, stride = 1, padding = 0, bias = False),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #out 1024x4x4\n",
    "    \n",
    "    nn.ConvTranspose2d(1024,512,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #out 512x8x8\n",
    "    \n",
    "    nn.ConvTranspose2d(512,256,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #out 256x16x16\n",
    "    \n",
    "    nn.ConvTranspose2d(256,128,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #out 128x32x32\n",
    "    \n",
    "    nn.ConvTranspose2d(128,64,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #out 64x64x64\n",
    "    \n",
    "    nn.ConvTranspose2d(64,32,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.LeakyReLU(0.1, inplace=True),\n",
    "    #out 32x128x128\n",
    "    \n",
    "    nn.ConvTranspose2d(32,3,kernel_size = 4, stride = 2, padding = 1, bias = False),\n",
    "    nn.Tanh()\n",
    "    #out 3x256x256\n",
    "    \n",
    ")\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = torch.randn(batch_size,latent_sz,1,1,)\n",
    "fake_images = generator(xb)\n",
    "show_images(fake_images)\n",
    "generator = to_device(generator,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_images,opt_d):\n",
    "    \n",
    "    opt_d.zero_grad()\n",
    "    real_preds = descriminator(real_images)\n",
    "    real_targets = torch.ones(real_images.size(0),1,device = device)\n",
    "    real_loss = F.binary_cross_entropy(real_preds,real_targets)\n",
    "    real_score = torch.mean(real_preds).item()\n",
    "    \n",
    "    latent = torch.randn(batch_size,latent_sz,1,1, device = device)\n",
    "    fake_images = generator(latent)\n",
    "    \n",
    "    fake_preds = descriminator(fake_images)\n",
    "    fake_targets = torch.zeros(fake_images.size(0),1,device = device)\n",
    "    fake_loss = F.binary_cross_entropy(fake_preds,fake_targets)\n",
    "    fake_score = torch.mean(fake_preds).item()\n",
    "    \n",
    "    loss = fake_loss+real_loss\n",
    "    loss.backward()\n",
    "    opt_d.step()\n",
    "    \n",
    "    return loss.item(),real_score,fake_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(opt_g):\n",
    "    opt_g.zero_grad()\n",
    "    latent = torch.randn(batch_size,latent_sz, 1,1, device = device)\n",
    "    images = generator(latent)\n",
    "    \n",
    "    targets = torch.ones(batch_size,1,device = device)\n",
    "    score = descriminator(images)\n",
    "    loss = F.binary_cross_entropy(score,targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    opt_g.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"gen\"\n",
    "os.makedirs(savedir, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(index, latent_tensors, show=True):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_fname = 'generated-images-{0:0=4d}e1.png'.format(index)\n",
    "    save_image(denorm(fake_images), os.path.join(savedir, fake_fname), nrow=8)\n",
    "    print('Saving', fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(batch_size, latent_sz, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_samples(0,fixed_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, start_idx = 1):\n",
    "    loss_d =[]\n",
    "    loss_g = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    \n",
    "    opt_d = torch.optim.Adam(descriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for img, _ in tqdm(dataload):\n",
    "            \n",
    "            loss, real_score, fake_score = train_discriminator(img, opt_d)\n",
    "            lossg = train_generator(opt_g)\n",
    "            \n",
    "        loss_d.append(loss)\n",
    "        loss_g.append(lossg)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "        \n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss, lossg, real_score, fake_score))\n",
    "        \n",
    "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
    "        \n",
    "    return loss_g,loss_d,real_scores,fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "epochs = 60\n",
    "history = fit(epochs,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(),\"genweights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(descriminator.state_dict(),\"discweights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_test = torch.randn(latent_sz,1,1,device = device)\n",
    "image = generator(latent_test)\n",
    "plt.imshow(image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
